{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Challenge#1 - African Snake Antivenom Binding",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SETUP"
      ],
      "metadata": {
        "id": "tA6WnOxLhys7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXHQEtzWhrSS"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTS"
      ],
      "metadata": {
        "id": "096OX259iWmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random  ,os\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import gc\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "from torch import nn \n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "W47qxZSFiP8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Data"
      ],
      "metadata": {
        "id": "M1vXVtGQidCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"https://storage.googleapis.com/umojahack2022/train.csv\")\n",
        "test_df = pd.read_csv(\"https://storage.googleapis.com/umojahack2022/test.csv\")"
      ],
      "metadata": {
        "id": "Cqr6bjLjibur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.shape , test_df.shape)\n",
        "print('-----------')\n",
        "print(train_df.Toxin_UniprotID.nunique() , test_df.Toxin_UniprotID.nunique())"
      ],
      "metadata": {
        "id": "WmPo4VXQieuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS"
      ],
      "metadata": {
        "id": "7ueaRC5ji8be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_all(SEED_VAL=1):\n",
        "        random.seed(SEED_VAL)\n",
        "        np.random.seed(SEED_VAL)\n",
        "        torch.manual_seed(SEED_VAL)\n",
        "        torch.cuda.manual_seed_all(SEED_VAL)\n",
        "        os.environ['PYTHONHASHSEED'] = str(SEED_VAL)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "ok2pqKkeofGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def free_memory(sleep_time=0.1):\n",
        "    \"\"\" Black magic function to free torch memory and some jupyter whims \"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.synchronize()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    time.sleep(sleep_time)"
      ],
      "metadata": {
        "id": "Crw-7RjUdUz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_seq_column_map(train, test, col):\n",
        "    sequences = []\n",
        "    for seq in train[col]:\n",
        "        sequences.extend(list(seq))\n",
        "    for seq in test[col]:\n",
        "        sequences.extend(list(seq))\n",
        "    unique = np.unique(sequences)\n",
        "    return {k: v for k, v in zip(unique, range(len(unique)))}\n",
        "\n",
        "def get_column_map(train, test, col):\n",
        "    sequences = []\n",
        "    unique_values = pd.concat([train[col], test[col]]).unique().tolist()\n",
        "    return {k: v for k, v in zip(unique_values, range(len(unique_values)))}"
      ],
      "metadata": {
        "id": "GyEThHRCi4gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AntivenomChallengeDataSet(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        amino_acid_map,\n",
        "        antivenom_map,\n",
        "        data,\n",
        "        is_train,\n",
        "        label_name=None,\n",
        "      ):\n",
        "        self.amino_acid_map = amino_acid_map\n",
        "        self.antivenom_map = antivenom_map\n",
        "        self.data = data\n",
        "        self.is_train = is_train\n",
        "        self.label_name = label_name\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) \n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        kmer_seq = torch.as_tensor([self.amino_acid_map[e] for e in list(row[\"Toxin_Kmer\"])])\n",
        "        antivenom = torch.as_tensor(self.antivenom_map[row[\"Antivenom\"]])\n",
        "        position_start = torch.as_tensor(row[\"Kmer_Position_start\"])\n",
        "        position_end = torch.as_tensor(row[\"Kmer_Position_end\"])\n",
        "        \n",
        "        inputs = {\n",
        "            \"K_mer\": kmer_seq,\n",
        "            \"antivenom\": antivenom,\n",
        "            \"position_start\": position_start,\n",
        "            \"position_end\": position_end,\n",
        "        }\n",
        "\n",
        "        if self.is_train: \n",
        "            return inputs, torch.as_tensor([row[self.label_name]])\n",
        "        return inputs"
      ],
      "metadata": {
        "id": "9V8_CoFXi-V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model):\n",
        "        super(ResidualLSTM, self).__init__()\n",
        "        self.LSTM=nn.LSTM(d_model, d_model, num_layers=1, bidirectional=True,batch_first=True)\n",
        "        self.linear1=nn.Linear(d_model*2, d_model*4)\n",
        "        self.linear2=nn.Linear(d_model*4, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res=x\n",
        "        x, _ = self.LSTM(x)\n",
        "        x=F.relu(self.linear1(x))\n",
        "        x=self.linear2(x)\n",
        "        x=res+x\n",
        "        return x\n",
        "\n",
        "class SimpleSeqModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Credits : INstadeepStartNotebook & https://www.kaggle.com/code/shujun717/1-solution-lstm-cnn-transformer-1-fold\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        K_mer_emb_size,\n",
        "        K_mer_nunique,\n",
        "        antivenom_emb_size,\n",
        "        antivenom_unique,\n",
        "        max_Position_start,\n",
        "        Position_start_emb_size,\n",
        "    ): \n",
        "        super().__init__()\n",
        "        self.K_mer_emb_size = K_mer_emb_size        \n",
        "        self.K_mer_nunique = K_mer_nunique                \n",
        "        self.antivenom_emb_size = antivenom_emb_size  \n",
        "        self.antivenom_unique = antivenom_unique    \n",
        "        self.rnnlayers = 3\n",
        "        self.max_seq=None\n",
        "        self.nlayers=3\n",
        "        self.dropout=0\n",
        "        self.nheads=16\n",
        "\n",
        "        self.Kmer_emb_layer = nn.Embedding( num_embeddings=self.K_mer_nunique,embedding_dim=self.K_mer_emb_size,)\n",
        "        \n",
        "        embed_dim =self.K_mer_emb_size\n",
        "        self.pos_encoder = nn.ModuleList([ResidualLSTM(self.K_mer_emb_size) for i in range(self.rnnlayers)])\n",
        "        self.pos_encoder_dropout = nn.Dropout(self.dropout)\n",
        "        self.layer_normal = nn.LayerNorm(embed_dim)\n",
        "        encoder_layers = [nn.TransformerEncoderLayer(embed_dim, self.nheads, embed_dim*4, self.dropout) for i in range(self.nlayers)]\n",
        "        conv_layers = [nn.Conv1d(embed_dim,embed_dim,(self.nlayers-i)*2-1,stride=1,padding=0) for i in range(self.nlayers)]\n",
        "        deconv_layers = [nn.ConvTranspose1d(embed_dim,embed_dim,(self.nlayers-i)*2-1,stride=1,padding=0) for i in range(self.nlayers)]\n",
        "        layer_norm_layers = [nn.LayerNorm(embed_dim) for i in range(self.nlayers)]\n",
        "        layer_norm_layers2 = [nn.LayerNorm(embed_dim) for i in range(self.nlayers)]\n",
        "        self.transformer_encoder = nn.ModuleList(encoder_layers)\n",
        "        self.conv_layers = nn.ModuleList(conv_layers)\n",
        "        self.layer_norm_layers = nn.ModuleList(layer_norm_layers)\n",
        "        self.layer_norm_layers2 = nn.ModuleList(layer_norm_layers2)\n",
        "        self.deconv_layers = nn.ModuleList(deconv_layers)\n",
        "        self.pred = nn.Linear(embed_dim, 1)\n",
        "        self.downsample = nn.Linear(embed_dim*2,embed_dim)\n",
        "\n",
        "        self.Antivenom_emb = nn.Embedding(num_embeddings=self.antivenom_unique,embedding_dim=self.antivenom_emb_size,)\n",
        "        self.Position_start_emb = nn.Embedding(num_embeddings=max_Position_start,embedding_dim=Position_start_emb_size,)\n",
        "        self.Features = nn.Linear(in_features=self.antivenom_emb_size + Position_start_emb_size,out_features=128,)\n",
        "        self.Linear_1 = nn.Linear(in_features=1152,out_features=512,)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.Output = nn.Linear(in_features=self.Linear_1.out_features, out_features=1,)\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        kmer_emb = self.Kmer_emb_layer(inputs[\"K_mer\"])\n",
        "        for lstm in self.pos_encoder:\n",
        "            kmer_emb=lstm(kmer_emb)\n",
        "        kmer_emb = torch.squeeze(kmer_emb)\n",
        "        kmer_emb = self.pos_encoder_dropout(kmer_emb)\n",
        "        kmer_emb = self.layer_normal(kmer_emb)\n",
        "\n",
        "        for conv, transformer_layer, layer_norm1, layer_norm2, deconv in zip(self.conv_layers,self.transformer_encoder,self.layer_norm_layers,\n",
        "                                                                             self.layer_norm_layers2,self.deconv_layers):\n",
        "            #LXBXC to BXCXL\n",
        "            res=kmer_emb\n",
        "            kmer_emb=F.relu(conv(kmer_emb.permute(1,2,0)).permute(2,0,1))\n",
        "            kmer_emb=layer_norm1(kmer_emb)\n",
        "            kmer_emb=transformer_layer(kmer_emb)\n",
        "            kmer_emb=F.relu(deconv(kmer_emb.permute(1,2,0)).permute(2,0,1))\n",
        "            kmer_emb=layer_norm2(kmer_emb)\n",
        "            kmer_emb=res+kmer_emb\n",
        "        \n",
        "        antivenom_emb = self.Antivenom_emb(inputs[\"antivenom\"])\n",
        "        position_start_emb = self.Position_start_emb(inputs[\"position_start\"])\n",
        "        emb_features = torch.cat((antivenom_emb, position_start_emb), axis=1)\n",
        "        features = self.Features(emb_features)\n",
        "\n",
        "        emb = torch.cat((torch.squeeze(kmer_emb[:,1,:], 1), features), axis=1)\n",
        "        linear_1 = self.relu_1(self.Linear_1(emb))\n",
        "        output = self.Output(linear_1)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "vtpVbYMSjBBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_func(fold,train_data_loader,val_data_loader,model,loss_fn,optimizer,num_epochs,device,writer,early_stopping=5,):  \n",
        "    def get_score(y_true, y_pred):\n",
        "      return mean_squared_error(y_true, y_pred,squared=False)\n",
        "    \n",
        "    seed_all()\n",
        "    total_batches = len(train_data_loader)\n",
        "    total_batches_val = len(val_data_loader)\n",
        "    train_loss = []\n",
        "    \n",
        "    n_iter = 0\n",
        "    best_outputs = []\n",
        "    for epoch in range(num_epochs): \n",
        "        tqdm_bar = tqdm(train_data_loader, desc=f\"epoch {epoch}\", position=0) \n",
        "        old_val_loss = np.inf\n",
        "        wating = 0\n",
        "        model.train()\n",
        "        for batch_number, (X, y) in enumerate(tqdm_bar):\n",
        "            y = y.type(torch.FloatTensor).to(device)\n",
        "            X = {k: X[k].to(device) for k in X}\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            pred = model(X)\n",
        "            loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            \n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            loss = loss.item()\n",
        "            train_loss.append(loss)\n",
        "\n",
        "            writer.add_scalar(\"loss/train\", loss, n_iter)\n",
        "            n_iter += 1\n",
        "\n",
        "            if batch_number % 25 == 0: \n",
        "                tqdm_bar.set_postfix({\"train\": f\"{batch_number}/{total_batches} loss: {loss:.3} epoch loss: {np.mean(train_loss):.3}\",},)\n",
        "\n",
        "        ############## validation  ############## \n",
        "        val_tqdm_bar = tqdm(val_data_loader, desc=f\"epoch {epoch}\", position=0, leave=True,) \n",
        "        final_outputs = []\n",
        "        final_targets = []  \n",
        "        val_loss = []\n",
        "        model.eval()\n",
        "        with torch.no_grad(): \n",
        "            for batch_number, (X, y) in enumerate(val_tqdm_bar):\n",
        "                y = y.type(torch.FloatTensor).to(device)\n",
        "                X = {k: X[k].to(device) for k in X}\n",
        "                \n",
        "                pred = model(X)\n",
        "                final_outputs.append(pred.cpu().detach().numpy())\n",
        "                final_targets.append(y.cpu().numpy())\n",
        "                val_loss.append(loss_fn(pred, y).item())\n",
        "\n",
        "                writer.add_scalar(\"loss/validation\", np.random.random(), n_iter)\n",
        "                if batch_number % 25 == 0: \n",
        "                    val_tqdm_bar.set_postfix({\"valid\": f\"{batch_number}/{total_batches_val} val loss: {np.mean(val_loss):.3}\"},)\n",
        "        \n",
        "        new_val_loss = np.mean(val_loss)\n",
        "        final_targets = np.concatenate(final_targets)\n",
        "        final_outputs = np.concatenate(final_outputs)\n",
        "        \n",
        "        scoree = get_score(final_targets,final_outputs)\n",
        "        print('Validation RMSE for this epoch',scoree)\n",
        "        print()\n",
        "        if new_val_loss > old_val_loss:\n",
        "            wating += wating\n",
        "        else:\n",
        "            old_val_loss = new_val_loss\n",
        "            best_outputs = final_outputs\n",
        "            torch.save(model, f\"model_fold{fold}.pth\")\n",
        "        if wating > early_stopping:\n",
        "            break\n",
        "    return best_outputs"
      ],
      "metadata": {
        "id": "6E7hoiNt1-TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer :\n",
        "  def __init__(self,train_df,test_df) :\n",
        "    self.train_df = train_df\n",
        "    self.test_df = test_df\n",
        "    self.n_splits = 10\n",
        "\n",
        "    #Data loader params\n",
        "    self.batch_size = 512\n",
        "    self.num_workers = 0\n",
        "    self.shuffle = True\n",
        "    self.drop_last = False\n",
        "\n",
        "    # model params\n",
        "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    self.num_epochs = 25\n",
        "    self.early_stopping = 5\n",
        "    self.lr = 1e-4\n",
        "\n",
        "  def seed_all(self):\n",
        "        random.seed(self.SEED_VAL)\n",
        "        np.random.seed(self.SEED_VAL)\n",
        "        torch.manual_seed(self.SEED_VAL)\n",
        "        torch.cuda.manual_seed_all(self.SEED_VAL)\n",
        "        os.environ['PYTHONHASHSEED'] = str(self.SEED_VAL)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "  def get_score(self,y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred,squared=False)\n",
        "\n",
        "  def Split(self,Train) :\n",
        "    X      = Train[[\"Toxin_UniprotID\"]]\n",
        "    y      = Train['Signal']\n",
        "    groups = Train[\"Toxin_UniprotID\"]\n",
        "    \n",
        "    kf = GroupKFold(n_splits=self.n_splits)\n",
        "    Train[\"folds\"]=-1   \n",
        "    for fold, (_, val_index) in enumerate(kf.split(X, y,groups=groups)):\n",
        "          Train.loc[val_index, \"folds\"] = fold\n",
        "    return Train\n",
        "\n",
        "  def TrainKfold(self,) :\n",
        "    seed_all()\n",
        "    amino_acid_map = get_seq_column_map(self.train_df, self.test_df, \"Toxin_Kmer\")\n",
        "    antivenom_map = get_column_map(self.train_df, self.test_df, \"Antivenom\")\n",
        "    max_Position_start = pd.concat([self.train_df[[\"Kmer_Position_start\"]], self.test_df[[\"Kmer_Position_start\"]]]).Kmer_Position_start.max()+1\n",
        "\n",
        "    self.train_df = self.Split(self.train_df)\n",
        "    oof = np.zeros((self.train_df.shape[0],1))\n",
        "\n",
        "    for fold in range(self.n_splits):\n",
        "      train_split_df = self.train_df[self.train_df.folds != fold]\n",
        "      val_split_df   = self.train_df[self.train_df.folds == fold]\n",
        "      val_split_df_index   = self.train_df[self.train_df.folds == fold].index.tolist()\n",
        "\n",
        "      train_dataset = AntivenomChallengeDataSet(amino_acid_map=amino_acid_map,antivenom_map=antivenom_map,\n",
        "                                                data=train_split_df,is_train=True,label_name=\"Signal\",)\n",
        "      val_dataset = AntivenomChallengeDataSet(amino_acid_map=amino_acid_map,antivenom_map=antivenom_map,\n",
        "                                              data=val_split_df,is_train=True,label_name=\"Signal\",)\n",
        "      test_dataset = AntivenomChallengeDataSet(amino_acid_map=amino_acid_map,antivenom_map=antivenom_map,data=test_df,is_train=False,)\n",
        "\n",
        "      train_data_loader = DataLoader(dataset=train_dataset,batch_size=self.batch_size,shuffle=self.shuffle, num_workers=self.num_workers,drop_last=self.drop_last, )\n",
        "      val_data_loader = DataLoader(dataset=val_dataset,batch_size=self.batch_size,shuffle=False,num_workers=self.num_workers,drop_last=False,  )\n",
        "      test_data_loader= DataLoader(dataset=test_dataset,batch_size=self.batch_size,shuffle=False,num_workers=self.num_workers,drop_last=False,)\n",
        "\n",
        "      \n",
        "\n",
        "      model = SimpleSeqModel(K_mer_emb_size=1024,K_mer_nunique=len(amino_acid_map),\n",
        "                            antivenom_emb_size=128,antivenom_unique=len(antivenom_map),\n",
        "                            max_Position_start=max_Position_start,Position_start_emb_size=64,)\n",
        "\n",
        "      loss_fn = nn.MSELoss()\n",
        "      model = model.to(self.device)\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)\n",
        "      writer = SummaryWriter()\n",
        "      writer.add_graph(model, {k: v.to(self.device) for k, v in next(iter(train_data_loader))[0].items()})\n",
        "\n",
        "      \n",
        "\n",
        "      oof_fold = train_func(fold,train_data_loader=train_data_loader,val_data_loader=val_data_loader,\n",
        "                            model=model,loss_fn=loss_fn,optimizer=optimizer,\n",
        "                            num_epochs=self.num_epochs,device=self.device,writer = writer , early_stopping=self.early_stopping,)\n",
        "\n",
        "      oof[val_split_df_index] = oof_fold\n",
        "\n",
        "    return oof\n",
        "\n",
        "  def INFERENCE(self,) :\n",
        "    seed_all()\n",
        "    amino_acid_map = get_seq_column_map(self.train_df, self.test_df, \"Toxin_Kmer\")\n",
        "    antivenom_map = get_column_map(self.train_df, self.test_df, \"Antivenom\")\n",
        "    max_Position_start = pd.concat([self.train_df[[\"Kmer_Position_start\"]], self.test_df[[\"Kmer_Position_start\"]]]).Kmer_Position_start.max()+1\n",
        "    test_dataset = AntivenomChallengeDataSet(amino_acid_map=amino_acid_map,antivenom_map=antivenom_map,data=self.test_df,is_train=False,)\n",
        "    test_data_loader= DataLoader(dataset=test_dataset,batch_size=self.batch_size,shuffle=False,num_workers=self.num_workers,drop_last=False,)\n",
        "    final_preds = []\n",
        "    for fold in range(self.n_splits):\n",
        "      path= f\"model_fold{fold}.pth\"\n",
        "      model = torch.load(path).to(self.device)\n",
        "      tqdm_bar = tqdm(test_data_loader, desc=f\"Inference-Fold{fold}\", position=0, leave=True) \n",
        "      total_batches = len(tqdm_bar)\n",
        "\n",
        "      preds = []\n",
        "      with torch.no_grad():\n",
        "          for batch_number, X in enumerate(tqdm_bar):\n",
        "              X= {k: X[k].to(self.device) for k in X}\n",
        "              pred = model(X)\n",
        "              preds.append(pred.cpu().numpy())\n",
        "\n",
        "          preds = np.concatenate(preds).reshape((-1))\n",
        "      final_preds.append(preds)\n",
        "    return np.mean(final_preds,0)"
      ],
      "metadata": {
        "id": "_ZDfuECwl2Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AssazzinTrainer = Trainer(train_df,test_df)"
      ],
      "metadata": {
        "id": "ubqossgn4j1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "free_memory(sleep_time=0.1)\n",
        "import gc ; gc.collect()"
      ],
      "metadata": {
        "id": "IVY-yUcj4jzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OOF = AssazzinTrainer.TrainKfold()"
      ],
      "metadata": {
        "id": "HRkHazfZ4jxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('OOF RMSE  :',AssazzinTrainer.get_score(train_df['Signal'],OOF))"
      ],
      "metadata": {
        "id": "o_Nl0DIB6XVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INFERENCE & SUBMISSION"
      ],
      "metadata": {
        "id": "lTLtPTwunA-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = AssazzinTrainer.INFERENCE()"
      ],
      "metadata": {
        "id": "lG_QMOoJF9Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission=test_df[[\"ID\"]]\n",
        "sample_submission[\"Signal\"] = np.clip(test_pred,train_df['Signal'].min(),train_df['Signal'].max())\n",
        "sample_submission.to_csv(\"AssazzinGoodBaseline_Complex_v3.csv\",index=False)"
      ],
      "metadata": {
        "id": "uGsLY7X54lFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission[\"Signal\"].hist()"
      ],
      "metadata": {
        "id": "34eEOKd0Gm50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "2d0e9c6a-44f1-4617-a494-5454aca9d8d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc6404fa1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}